{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "from scipy import signal\n",
    "import sounddevice as sd\n",
    "\n",
    "\"Machine learning tools\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import recall_score \n",
    "import pickle\n",
    "\n",
    "\"Self created functions\"\n",
    "from utils_ import getclass, getname, gen_allpath, plot_audio, plot_specgram, get_accuracy, show_confusion_matrix, plot_decision_boundaries\n",
    "from AudioUtil_And_Dataset_student import AudioUtil, SoundDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful functions to select, read and play the dataset sounds are provided in the ``utils_`` and ``AudioUtil_And_Dataset`` folder. <br>\n",
    "\n",
    "As for the H1, you will have to fill some short pieces of code, as well as answer some questions. We already created cells for you to answer the questions to ensure you don't forget it ;). <br>\n",
    "You will find the zones to be briefly filled  with a ``### TO COMPLETE`` in the cells below.\n",
    "\n",
    "<font size=6 color=#009999> 2. Training and Evaluating models on audio signals [~1h30-2h] </font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Le chemin d’accès spécifié est introuvable: 'C:/Users/MaudTS/Documents/ESC-50'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m### TO COMPLETE\u001b[39;00m\n\u001b[0;32m      2\u001b[0m path2dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/MaudTS/Documents/ESC-50\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Write your path to the dataset here!\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m allclassnames, allpath_mat \u001b[38;5;241m=\u001b[39m \u001b[43mgen_allpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath2dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelect only some classes for the classification\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m sel_class \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m14\u001b[39m,\u001b[38;5;241m40\u001b[39m,\u001b[38;5;241m41\u001b[39m,\u001b[38;5;241m49\u001b[39m] \n",
      "File \u001b[1;32m~\\Desktop\\Cours M\\projet elec\\H_six\\H1_H3a-master\\utils_.py:50\u001b[0m, in \u001b[0;36mgen_allpath\u001b[1;34m(folder)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgen_allpath\u001b[39m(folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset_ESC-50\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m    Create a matrix with path names of height H=50classes and W=40sounds per class\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m    and an array with all class names.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     classpath \u001b[38;5;241m=\u001b[39m [f\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mis_dir()]\n\u001b[0;32m     51\u001b[0m     classpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(classpath)\n\u001b[0;32m     53\u001b[0m     allpath \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(classpath)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Le chemin d’accès spécifié est introuvable: 'C:/Users/MaudTS/Documents/ESC-50'"
     ]
    }
   ],
   "source": [
    "### TO COMPLETE\n",
    "path2dataset = \"C:/Users/MaudTS/Documents/ESC-50\" # Write your path to the dataset here!\n",
    "allclassnames, allpath_mat = gen_allpath(path2dataset)\n",
    "\n",
    "\"Select only some classes for the classification\"\n",
    "sel_class = [12,14,40,41,49] \n",
    "nonsel_class = np.delete(np.arange(allpath_mat.shape[0]), sel_class)\n",
    "\n",
    "allpath_sel = np.array([allpath_mat[idx,:] for idx in sel_class])\n",
    "allpath_nonsel = np.array([allpath_mat[idx,:] for idx in nonsel_class])\n",
    "classnames = np.array([allclassnames[idx] for idx in sel_class])\n",
    "sel_class_ids = np.arange(len(sel_class))\n",
    "all_sel_class_ids = np.repeat(sel_class_ids, 40)\n",
    "data_path = allpath_sel.reshape(-1)\n",
    "\n",
    "print('The selected classes are {}'.format(classnames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In H1, it was not made explicit what we choose as input for the classification model, a.k.a. ``feature vector`` (it was shown in the illustration). <br>\n",
    "The objective is, on the transmitter side, to compute a feature vector containing enough information about the audio signal we want to classify, but not too much in order to limit the data which has to be transmitted wirelessly. This is why in H1 we implemented the ``Hz2Mel`` conversion, a very simple compression of the frequency content. <br>\n",
    "The feature vector we will use here simply consists in taking the first 10 columns of the melspectrogram, corresponding to ~0.5s, then reshaping it as a vector. This means each feature vector contains ``200`` coefficients, with 20 mels for 10 columns.  <br>\n",
    "\n",
    "Once the feature vector has been recovered on the receiver side, we can apply any computation on it to guess the right class this sound belongs to. For today, we will simply reuse the simple KNN and LDA classifiers and look at what we already get.  \n",
    "\n",
    "<font size=5 color=#009999> 2.1. Creation of the dataset </font> <br>\n",
    "\n",
    "``SoundDS`` is a class defined in ``AudioUtil_And_Dataset.py``. <br>\n",
    "The functions ``__len__`` and ``__getitem__`` are implemented, meaning you can call :\n",
    "- ``len(myds)`` to get the number of sounds in it.\n",
    "- ``myds[i][j]`` to get the melspectrogram of the ``j``-th sound from class ``i``. <br>\n",
    "\n",
    "Two other useful functions are provided:\n",
    "- ``get_audiosignal`` returning the temporal audiosignal at the specified index.\n",
    "- ``display`` playing the sound and showing the associated mel-spectrogram at the specified index.\n",
    "\n",
    "<font size=3 color=#FF0000> Important :</font> <br>\n",
    "Before being able to run the cells below, you will have to reuse your functions from H1 to fill the missing lines in ``AudioUtil_And_Dataset.py`` at ``###TO COMPLETE`` locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "\"Creation of the dataset\"\n",
    "myds = SoundDS(all_sel_class_ids, Nft=512, nmel=20, duration=750, shift_pct=0.0, data_path=data_path, allpath_mat=allpath_mat)\n",
    "\n",
    "\"Some attributes...\"\n",
    "myds.nmel\n",
    "myds.allpath_mat\n",
    "myds.class_ids\n",
    "myds.data_path\n",
    "myds.duration\n",
    "myds.shift_pct\n",
    "myds.sr\n",
    "myds.data_aug\n",
    "myds.ncol\n",
    "\n",
    "idx = 51\n",
    "myds.display(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the cell above many time, you should notice it is always the beginning of the sound that is taken for creating the feature vector. ``shift_pct`` meaning *shift percentage* allows to roll the audio signal with a random factor upper bounded by this value. Change ``shift_pct`` to ``0.2`` and observe what happens.\n",
    "### ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "\"Random split of 70:30 between training and validation\"\n",
    "train_pct = 0.7\n",
    "\n",
    "featveclen = len(myds[0][0]) # number of items in a feature vector\n",
    "num_items = len(myds) # number of sounds in the dataset\n",
    "num_sounds = len(myds.allpath_mat[0,:]) # number of sounds in each class\n",
    "num_classes = int(num_items/num_sounds) # number of classes\n",
    "num_learn = round(num_sounds * train_pct) # number of sounds among num_sounds for training \n",
    "\n",
    "data_aug_factor = 1\n",
    "sel_class_ids = np.arange(len(sel_class))\n",
    "sel_class_ids_aug = np.repeat(sel_class_ids, num_sounds*data_aug_factor)\n",
    "\n",
    "\"Compute the matrixed dataset, this takes some seconds, but you can then reload it by commenting this loop and decommenting the np.load below\"\n",
    "X = np.zeros((data_aug_factor*num_classes*num_sounds, featveclen))\n",
    "for s in range(data_aug_factor):\n",
    "    for i, featvec in enumerate(myds):\n",
    "        X[s*num_classes*num_sounds+i,:] = featvec[0]\n",
    "np.save(\"feature_matrix_2D.npy\", X)\n",
    "\n",
    "# X = np.load(\"feature_matrix_2D.npy\")\n",
    "\n",
    "\"Labels\"\n",
    "y = sel_class_ids_aug.copy()\n",
    "\n",
    "print('Shape of the feature matrix : {}'.format(X.shape))\n",
    "print('Number of labels : {}'.format(len(y)))\n",
    "\n",
    "print('Remember the convention shown for the toy example, the feature vectors are arranged on the rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> 2.2. Classification </font> <br>\n",
    "\n",
    "For now we have only prepared the dataset, it remains to feed it to the classifiers. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "K = 10 # Number of neighbours for the KNN\n",
    "model_knn = KNeighborsClassifier(n_neighbors=K, weights='distance', algorithm='auto', metric='minkowski') #We explicitly write the default parameters of this KNN classifier once so that you know they exist and can be changed\n",
    "\n",
    "model_lda = LDA(solver='svd', shrinkage=None, priors=None, n_components=None, store_covariance=False, tol=0.0001, covariance_estimator=None) #We explicitly write the default parameters of this LDA classifier once so that you know they exist and can be changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the toy example, we keep the ``accuracy`` and ``confusion matrix`` as performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "\"Shuffle then split the dataset into training and testing subsets\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y) # random_state=1\n",
    "print('Shape of the training matrix : {}'.format(X_train.shape))\n",
    "print('Number of training labels : {}'.format(len(y_train)))\n",
    "\n",
    "model_knn.fit(X_train, y_train)\n",
    "model_lda.fit(X_train, y_train)\n",
    "\n",
    "prediction_knn = model_knn.predict(X_test)\n",
    "prediction_lda = model_lda.predict(X_test)\n",
    "accuracy_knn = get_accuracy (prediction_knn, y_test)\n",
    "accuracy_lda = get_accuracy (prediction_lda, y_test)\n",
    "\n",
    "print('Accuracy of KNN with fixed train/validation sets : {:.1f}%'.format(100*accuracy_knn))\n",
    "show_confusion_matrix (prediction_knn, y_test, classnames)\n",
    "print('Accuracy of LDA with fixed train/validation sets : {:.1f}%'.format(100*accuracy_lda))\n",
    "show_confusion_matrix (prediction_lda, y_test, classnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**: \n",
    "- What would be the expected accuracy if the label predictions were picked at random?\n",
    "- What do you observe in this confusion matrix? Reapply the ``train_test_split`` and tell if your observations are robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO COMPLETE\n",
    "1. 20% \n",
    "2. The diagonal values are not high compared to the other values. The results obtained are not very good. When we reapply the ``train_test_split``, the accuracy obtained varies a lot. Our observations are thus not robust. ``train_test_split`` permet de séparer les données sélectionnées pour le LEARNING en training set et validation set et le fait de manière random. Cela a donc comme conséquence qu'on fait le training sur des données différentes à chaque fois, résultant en des résultats de précision différente. \n",
    "ATTENTION : KNN ne choisi presque jamais 'HANDSAW'. Raison : voir dicision spatiale des classes, la partie handasw est toute petite ET au centre de toutes les autres. Le classifier va donc facilement classifier les points handsaw dasn des mauvaises catégories. \n",
    "On peut d'ailleurs voir plus bas que la précison des deux classificateurs KNN et LDA peut varier fortement. En effet, elles ont une grande variance (std)\n",
    "Une fois que nous faisons une cross validation, on ne devrait plus avoir ce problème. \n",
    "On peut voir que la précision au KNN ne varie plus beaucoup avec le hyperparamètre K. En effet les clusters sont bien répartis (20 dimensions). Dépend de la nature du data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with the index ``ind`` to pick feature vectors in the dataset ``myds``, listen to the audio associated to the feature vector, and check if you would have been able to predict the right class by your own. Then compare with the prediction given by your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "ind = 1\n",
    "myds.display(ind)\n",
    "thisfv = myds[ind][0].reshape(-1)\n",
    "prediction_knn = model_knn.predict([thisfv])\n",
    "print('Class predicted by the model:', classnames[prediction_knn][0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, when training a model and comparing different settings, there is a risk that we will end up choosing optimal parameters that only renders good result on our specific case of training and validation set, but ``do not generalize well for additional data``. This is called ``overfitting on the validation set``. To alleviate this, we can perform ``cross-validation (CV)``. A basic approach named ``K-fold CV`` involves partitioning the dataset in ``K`` \"folds\" (subsets) and repetitvely do the following procedure:\n",
    "\n",
    "- Train the model using `K-1` folds as the training data.\n",
    "- Test the model using the last fold as the validation data.\n",
    "\n",
    "The overall performance on each fold is then averaged to obtain the final performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits,shuffle=True)\n",
    "\n",
    "accuracy_knn = np.zeros((n_splits,))\n",
    "accuracy_lda = np.zeros((n_splits,))\n",
    "for k, idx in enumerate(kf.split(X_train,y_train)):\n",
    "  (idx_learn, idx_val) = idx\n",
    "  model_knn.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "  prediction_knn = model_knn.predict(X_train[idx_val])\n",
    "  accuracy_knn[k] = get_accuracy(prediction_knn, y_train[idx_val])\n",
    "\n",
    "  model_lda.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "  prediction_lda = model_lda.predict(X_train[idx_val])\n",
    "  accuracy_lda[k] = get_accuracy(prediction_lda, y_train[idx_val])\n",
    "\n",
    "print('Mean accuracy of KNN with 5-Fold CV: {:.1f}%'.format(100*accuracy_knn.mean()))\n",
    "print('Std deviation in accuracy of KNN with 5-Fold CV: {:.1f}%'.format(100*accuracy_knn.std()))\n",
    "\n",
    "print('Mean accuracy of LDA with 5-Fold CV: {:.1f}%'.format(100*accuracy_lda.mean()))\n",
    "print('Std deviation in accuracy of LDA with 5-Fold CV: {:.1f}%'.format(100*accuracy_lda.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the upper analysis, we fixed ``K`` for the KNN. This is called an ``hyperparameter`` of the classification model. Let us now have a look at the effect of this hyperparameter!  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "Ks = np.arange(6,50, 1)\n",
    "accuracies_knn = np.zeros((len(Ks), n_splits))\n",
    "for i,K in enumerate(Ks):\n",
    "    model_knn = KNeighborsClassifier(n_neighbors=K, weights='distance') \n",
    "    for k, idx in enumerate(kf.split(X_train,y_train)):\n",
    "            (idx_learn, idx_val) = idx\n",
    "            model_knn.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "            prediction_knn = model_knn.predict(X_train[idx_val])\n",
    "            accuracies_knn[i,k] = get_accuracy(prediction_knn, y_train[idx_val])\n",
    "means_knn = accuracies_knn.mean(axis=1)\n",
    "stds_knn = accuracies_knn.std(axis=1)\n",
    "\n",
    "\"Plot\"\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(Ks, means_knn, '.-b', label='KNN')\n",
    "plt.fill_between(Ks,means_knn-stds_knn,means_knn+stds_knn,alpha=0.2,color='b')\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> 2.5. Scale mismatch and countermeasure </font> <br>\n",
    "\n",
    "In real conditions, you will most probably have a different scale between the feature vectors used for training (in simulation) and the ones you feed in your model to make predictions.\n",
    "This scale mismatch between model training and prediction is difficult to prevent because it depends on multiple factors such as the audio source power, its distance to the microphone, the telecommunication distance. <br>\n",
    "\n",
    "Play with the ``dB_mismatch`` variable here below and observe its effect on the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "dB_mismatch = -10 # Play with this value default : 0 \n",
    "# db_mismatch permet de voir si l'amplitude (augementation ou diminution) impacte l'efficacité de la classification\n",
    "#La réponse est OUI du cp faut normaliser\n",
    "X_val_scaled = X_train[idx_val]*10**(-dB_mismatch/20)\n",
    "\n",
    "model_knn = KNeighborsClassifier(n_neighbors=10) \n",
    "model_knn.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "prediction_knn = model_knn.predict(X_val_scaled)\n",
    "show_confusion_matrix (prediction_knn, y_train[idx_val], classnames)\n",
    "accuracy_knn = get_accuracy (prediction_knn, y_train[idx_val])\n",
    "print('Accuracy of KNN: {:.1f}%'.format(100*accuracy_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest countermeasure we can think of is to normalise the feature vector (i.e. unitize its norm) prior to use, both for training and testing. Remember how this normalization could be visualized in ``H3a_toy_student.ipynb`` <br>\n",
    "Play again with the ``dB_mismatch`` variable here below and observe its effect on the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "dB_mismatch = -20 # Play with this value --> NE change rien \n",
    "\n",
    "X_learn_normalised = X_train[idx_learn]/ np.linalg.norm(X_train[idx_learn], axis=1, keepdims=True)\n",
    "model_knn = KNeighborsClassifier(n_neighbors=10, weights='distance') \n",
    "model_knn.fit(X_learn_normalised, y_train[idx_learn])\n",
    "\n",
    "X_val_scaled = X_train[idx_val]*10**(-dB_mismatch/20)\n",
    "X_val_normalised = X_val_scaled/ np.linalg.norm(X_val_scaled, axis=1, keepdims=True)\n",
    "\n",
    "prediction_knn = model_knn.predict(X_val_normalised)\n",
    "show_confusion_matrix (prediction_knn, y_train[idx_val], classnames)\n",
    "accuracy_knn = get_accuracy (prediction_knn, y_train[idx_val])\n",
    "print('Accuracy of KNN: {:.1f}%'.format(100*accuracy_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: \n",
    "- What will happen with this normalisation countermeasure when there is no sound around the microphone? Is this desirable? How could you deal with it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO COMPLETE\n",
    "Le bruit peut être considéré comme un signal. On va normaliser le bruit, ce qui n'est pas désirable. Comment résoudre ce problème ? Treshhold\n",
    "Si on estime qu'on entend rien (énergie, amplitude : seuil sur l'amplitude (chance non nulle que le bruit ait une grande valeur à un moment donné donc travailler avec l'énergie est une meilleure idée)) : alors on ne normalize pas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> 2.4. Dimensionality reduction </font> <br>\n",
    "\n",
    "It is sometimes good practice to reduce the dimensionality of a signal in order to get the main components of their distribution. A motivation is that usual norms behave counter-inuitively in high dimension. To reduce the dimensionality, we will use the ``Principal compenent analysis (PCA)`` proposed by sklearn. See the [associated Wikipedia page](https://en.wikipedia.org/wiki/Principal_component_analysis). Recall: the PCA consists in reducing the dimensionality of data vectors encoded in $\\boldsymbol X \\in \\mathbb R^{d\\times N}$ to only $p \\ll d$ dimensions as\n",
    "\n",
    "$$\n",
    "    \\boldsymbol Y = \\boldsymbol V_p^\\top \\boldsymbol X \\in \\mathbb R^{p\\times N},\n",
    "$$\n",
    "\n",
    "where the SVD of the covariance matrix writes as $\\hat{\\boldsymbol\\Sigma}_{\\boldsymbol X} = \\frac{1}{d} \\boldsymbol{XX}^\\top = \\boldsymbol{U\\Sigma V}^\\top$, and $\\boldsymbol V_p$ is the subselection of the first $p$ columns of $\\boldsymbol V$. \n",
    "\n",
    "For our application, reducing the dimensionality of the data can be helpful for compressing the packet size to be transmitted wirelessly. Indeed, once learned during training, $\\boldsymbol V_p$ can be hardcoded on the transmitter side.\n",
    "\n",
    "Starting with a PCA to 2D for visualization, see how hard it is to separate the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "n=2 # Number of principal components kept\n",
    "pca = PCA(n_components=n,whiten=True)\n",
    "X_learn_reduced = pca.fit_transform(X_train[idx_learn])\n",
    "X_val_reduced = pca.transform(X_train[idx_val])\n",
    "\n",
    "print('Shape of the reduced training matrix : {}'.format(X_learn_reduced.shape))\n",
    "\n",
    "K = 10\n",
    "model_knn = KNeighborsClassifier(n_neighbors=K)\n",
    "model_knn.fit(X_learn_reduced, y_train[idx_learn])\n",
    "prediction_knn = model_knn.predict(X_val_reduced)\n",
    "accuracy_knn = get_accuracy(prediction_knn, y_train[idx_val])\n",
    "\n",
    "model_lda = LDA()\n",
    "model_lda.fit(X_learn_reduced, y_train[idx_learn])\n",
    "prediction_lda = model_lda.predict(X_val_reduced)\n",
    "accuracy_lda = get_accuracy(prediction_lda, y_train[idx_val])\n",
    "\n",
    "fig = plt.figure()\n",
    "axs = [fig.add_axes([0.0, 0.0, 0.4, 0.9]), fig.add_axes([0.6, 0.0, 0.4, 0.9])]\n",
    "plot_decision_boundaries(X_learn_reduced,y_train[idx_learn],ax=axs[0],model=model_knn,legend=classnames,title='KNN')\n",
    "plot_decision_boundaries(X_learn_reduced,y_train[idx_learn],ax=axs[1],model=model_lda,legend=classnames,title='LDA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: \n",
    "- From the decision boundaries shown here above, can you explain why the ``handsaw`` class is less often chosen than the other classes for the ``KNN`` classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parce que la classe Handsaw se trouve au centre des autres classes. Ce qui n'est pas le cas en LDA. EN 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "n=5 # Number of principal components kept\n",
    "pca = PCA(n_components=n,whiten=True)\n",
    "X_learn_reduced = pca.fit_transform(X_train[idx_learn])\n",
    "X_val_reduced = pca.transform(X_train[idx_val])\n",
    "\n",
    "print('Shape of the reduced learning matrix : {}'.format(X_learn_reduced.shape))\n",
    "\n",
    "K = 10\n",
    "model_knn = KNeighborsClassifier(n_neighbors=K, weights='distance')\n",
    "model_knn.fit(X_learn_reduced, y_train[idx_learn])\n",
    "prediction_knn = model_knn.predict(X_val_reduced)\n",
    "accuracy_knn = get_accuracy(prediction_knn, y_train[idx_val])\n",
    "\n",
    "model_lda = LDA()\n",
    "model_lda.fit(X_learn_reduced, y_train[idx_learn])\n",
    "prediction_lda = model_lda.predict(X_val_reduced)\n",
    "accuracy_lda = get_accuracy(prediction_lda, y_train[idx_val])\n",
    "\n",
    "print('Accuracy of the KNN : {:.1f}%'.format(100*accuracy_knn))\n",
    "show_confusion_matrix(prediction_knn, y_train[idx_val], classnames)\n",
    "print('Accuracy of the LDA : {:.1f}%'.format(100*accuracy_lda))\n",
    "show_confusion_matrix(prediction_lda, y_train[idx_val], classnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> 2.5. Analysis of the hyperparameters </font> <br>\n",
    "\n",
    "Finally, we can inspect the influence of ``hyperparameters`` as we did for the toy example. <br>\n",
    "Here we consider both ``K`` and the number of principal components ``n``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "Ks = np.arange(1,10)\n",
    "n_comps = np.arange(2, 15) # number of principal components kept for the PCA\n",
    "accuracies_knn = np.zeros( (len(Ks), len(n_comps)) )\n",
    "accuracies_lda = np.zeros(len(n_comps)) \n",
    "\n",
    "for j, n in enumerate(n_comps):\n",
    "    for idx_learn, idx_val in kf.split(X_train,y_train):\n",
    "        pca = PCA(n_components=n,whiten=True)\n",
    "        X_learn_reduced = pca.fit_transform(X_train[idx_learn])\n",
    "        X_val_reduced = pca.transform(X_train[idx_val])\n",
    "        for i,K in enumerate(Ks):\n",
    "            model_knn = KNeighborsClassifier(n_neighbors=K)\n",
    "            model_knn.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "            prediction_knn = model_knn.predict(X_train[idx_val])\n",
    "            accuracies_knn[i,j] += get_accuracy(prediction_knn, y_train[idx_val])\n",
    "        \n",
    "        model_lda = LDA()\n",
    "        model_lda.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "        prediction_lda = model_lda.predict(X_train[idx_val])\n",
    "        accuracies_lda[j] += get_accuracy(prediction_lda, y_train[idx_val])\n",
    "\n",
    "accuracies_knn /= n_splits\n",
    "accuracies_lda /= n_splits\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "axs = [fig.add_axes([0.0, 0.0, 0.4, 0.9]), fig.add_axes([0.6, 0.0, 0.4, 0.9])]\n",
    "\n",
    "im0 = axs[0].imshow(100*accuracies_knn, cmap='jet', origin='lower')\n",
    "cbar = fig.colorbar(im0, ax=axs[0])\n",
    "cbar.set_label('Accuracy (%)')\n",
    "axs[0].set_xlabel('n_PCA')\n",
    "axs[0].set_ylabel('K')\n",
    "axs[0].set_xticks(list(np.arange(len(n_comps))))\n",
    "axs[0].set_xticklabels(list(n_comps))\n",
    "axs[0].set_yticks(list(np.arange(len(Ks))))\n",
    "axs[0].set_yticklabels(list(Ks))\n",
    "axs[0].set_title('KNN')\n",
    "\n",
    "axs[1].plot(accuracies_lda*100)\n",
    "axs[1].set_xlabel('n_PCA')\n",
    "axs[1].set_ylabel('Accuracy (%)')\n",
    "axs[1].set_title('LDA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: \n",
    "- Do you observe some dependency of the accuracy on these parameters? If so, which one(s)? If not, discuss what it tells about the considered model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les points des différentes categories sont initialement répartis d'une manière si random (tous mélangés les uns aux autres) que si le k augmente de trop, le classificateur prendra juste des éléments au hasard et ca sera pas concluant. Mais du coup au lieu de vouloir optimiser le classificateur en lui meme, mieux vaut d'abord \"nettoyer\" nos données initiales (améliorer la répartition des points au sein de l'espace). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> 2.6. Augmenting the data </font> <br>\n",
    "\n",
    "In order to make our classifier more robust to some common transformations of the audio signal such as ``time shift``, ``scaling`` or ``AWGN``, we need to feed it with such transformations. A popular approach is to create new feature vectors based on transformed versions of the sounds from the original dataset, this is called ``data augmentation``. Data augmentation is also often used when there is few data to train a model. <br>\n",
    "\n",
    "The functions to augment your data are written in ``AudioUtil_And_Dataset.py``, we already implemented ``time_shift``, ``echo`` and ``spectro_aug_timefreq_masking`` for you. Try to implement ``scaling``, ``add_noise``, ``filter``, ``add_bg`` and even more data augmentation techniques if you want, and check their working in the cell below. <br>\n",
    "\n",
    "<u>Tip</u>: to avoid restarting the notebook kernel for each modification, you can temporarily insert the ``AudioUtil`` class in a new cell and make your tests until it is working as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "myds.data_aug = None # Ensure\n",
    "\n",
    "sound = allpath_mat[14,29]\n",
    "audio = AudioUtil.open(sound)\n",
    "\n",
    "AudioUtil.play(audio)\n",
    "audio2 = AudioUtil.resample(audio, 11025)\n",
    "audio2 = AudioUtil.pad_trunc(audio2, 5000)\n",
    "\n",
    "audio3 = AudioUtil.time_shift(audio2, 0.4)\n",
    "audio4 = AudioUtil.scaling(audio2) \n",
    "audio5 = AudioUtil.add_noise(audio2, sigma=1e-2)\n",
    "audio6 = AudioUtil.echo(audio2, 3)\n",
    "audio7 = AudioUtil.add_bg(audio2, allpath_nonsel)\n",
    "\n",
    "melspec = AudioUtil.melspectrogram(audio2, fs2=11025)\n",
    "melspec2 = AudioUtil.spectro_aug_timefreq_masking(melspec, max_mask_pct=0.1)\n",
    "\n",
    "\"Plot\"\n",
    "fig = plt.figure(figsize=(15,4))\n",
    "ax1 = fig.add_axes([0.05, 0.05, 0.28, 0.9])\n",
    "ax2 = fig.add_axes([0.38, 0.05, 0.28, 0.9])\n",
    "ax3 = fig.add_axes([0.7, 0.05, 0.28, 0.9])\n",
    "\n",
    "ax1.plot(audio2[0], label='Original')\n",
    "ax1.plot(audio3[0]+1, label='Time shifted')\n",
    "ax1.plot(audio4[0]+2, label='Rescaled')\n",
    "ax1.plot(audio5[0]+3, label='Noisy')\n",
    "ax1.plot(audio6[0]+4, label='With echos')\n",
    "ax1.plot(audio7[0]+5, label='With background sound')\n",
    "ax1.legend()\n",
    "\n",
    "plot_specgram(melspec, ax2, is_mel=True, title=getname(sound), tf = len(audio2[0])/audio2[1])\n",
    "ax2.set_title('Melspectrogram')\n",
    "plot_specgram(melspec2, ax3, is_mel=True, title=getname(sound), tf = len(audio2[0])/audio2[1])\n",
    "ax3.set_title('Corrupted melspectrogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a new augmented dataset and observe if the classification results improve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "myds.mod_data_aug(['add_bg'])\n",
    "sel_class_ids = np.arange(len(sel_class))\n",
    "sel_class_ids_aug = np.repeat(sel_class_ids, num_sounds*myds.data_aug_factor)\n",
    "\n",
    "\"Compute the matrixed dataset, this takes some seconds, but you can then reload it by commenting this loop and decommenting the np.load below\"\n",
    "X_aug = np.zeros((myds.data_aug_factor*num_classes*num_sounds, featveclen))\n",
    "for s in range(myds.data_aug_factor):\n",
    "    for i, featvec in enumerate(myds):\n",
    "        X_aug[s*num_classes*num_sounds+i,:] = featvec[0]\n",
    "np.save(\"feature_matrix_2D_aug.npy\", X_aug)\n",
    "\n",
    "# X_aug = np.load(\"feature_matrix_2D_aug.npy\")\n",
    "\n",
    "\"Labels\"\n",
    "y_aug = sel_class_ids_aug\n",
    "\n",
    "print('Shape of the feature matrix : {}'.format(X_aug.shape))\n",
    "print('Number of labels : {}'.format(len(y_aug)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "K = 10 # Number of neighbours\n",
    "model = KNeighborsClassifier(n_neighbors=K) \n",
    "\n",
    "accuracy_aug = np.zeros((n_splits,))\n",
    "for k, idx in enumerate(kf.split(X_aug,y_aug)):\n",
    "  (idx_train, idx_test) = idx\n",
    "  model.fit(X_aug[idx_train], y_aug[idx_train])\n",
    "  prediction_aug = model.predict(X_aug[idx_test])\n",
    "  accuracy_aug[k] = get_accuracy(prediction_aug, y_aug[idx_test])\n",
    "\n",
    "print('Mean accuracy with 5-Fold CV: {:.1f}%'.format(100*accuracy_aug.mean()))\n",
    "print('Std deviation in accuracy with 5-Fold CV: {:.1f}%'.format(100*accuracy_aug.std()))\n",
    "show_confusion_matrix(prediction_aug, y_aug[idx_test], classnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**:\n",
    "Can you see an improvement of the classification result compared to the non augmented dataset? <br>\n",
    "Try to interpret your answer by thinking about the distribution of points in a data space (as with the toy example), what does it imply to augment the data in terms of distribution of points in the data space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE\n",
    "# Answer the question above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> 2.7. Getting it all together </font> <br>\n",
    "\n",
    "Now that some aspects to be considered during the model training and analysis have been presented, it remains to train and save a final model that will be used for further predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE\n",
    "\n",
    "# [1] Create dataset and split it.\n",
    "# (optional) with data augmentation\n",
    "myds = SoundDS(all_sel_class_ids, Nft=512, nmel=20, duration=750, shift_pct=0.0, data_path=data_path, allpath_mat=allpath_mat)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = ...\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y) # random_state=1\n",
    "\n",
    "# [2] (optional) Data normalization\n",
    "X_learn_normalised = X_train/np.linalg.norm(X_train, axis=1, keepdims=True)\n",
    "# [3] (optional) dimensionality reduction. NOT DONE \n",
    "\n",
    "# [4] Model training and selection.\n",
    "K = 10 \n",
    "model = KNeighborsClassifier(n_neighbors=K, weights='distance', algorithm='auto', metric='minkowski') #We explicitly write the default parameters of this KNN classifier once so that you know they exist and can be changed\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits,shuffle=True)\n",
    "accuracy_knn = np.zeros((n_splits,))\n",
    "for k, idx in enumerate(kf.split(X_train,y_train)):\n",
    "  (idx_learn, idx_val) = idx\n",
    "  model.fit(X_learn_normalised[idx_learn], y_train[idx_learn])\n",
    "  prediction_knn = model.predict(X_train[idx_val])\n",
    "  accuracy_knn[k] = get_accuracy(prediction_knn, y_train[idx_val])\n",
    "\n",
    "# [5] Save the trained model, eventually the pca.\n",
    "filename = 'model.pickle'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "# [6] Evaluate the model\n",
    "\n",
    "X_test = X_test/np.linalg.norm(X_test, axis=1, keepdims=True)\n",
    "prediction = model.predict(X_test)\n",
    "show_confusion_matrix(prediction, y_test, classnames)\n",
    "accuracy = get_accuracy(prediction, y_test)\n",
    "recall = recall_score(y_test,prediction, average = None)\n",
    "print('Accuracy of KNN: {:.1f}%'.format(100*accuracy))\n",
    "print(recall)\n",
    "#print('Recall of KNN: {:.1f}%'.format(100*recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> 2.8. Debriefing </font> <br>\n",
    "**Questions** : \n",
    "\n",
    "1) from what we have done in this notebook, can you already identify some weaknesses in the feature vector computation and classification pipeline? You can make a list here below, and eventually write some short ideas for improvement. This will help you later :)\n",
    "2) Do you remember what is the time duration of a feature vector? What happens if no sound is produced during the acquisition time of a feature vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE\n",
    "# Answer the question above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "46df200377d403be22c796785365123e6a374b5da08e8292e6b2afda659c5a28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
